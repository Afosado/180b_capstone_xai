<!DOCTYPE html>
<html>

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://www.w3schools.com/lib/w3-theme-teal.css">
<link rel="stylesheet" href="main.css">
<body>

<div class="w3-card-4">
<div class="w3-container w3-theme w3-card" style="text-align:center">
  <h1><strong>Using Attention Maps and Counterfactuals to Explain Caption Generation Models</strong></h1>
</div>
<br> <br> <br>
<div style="text-align:center;">  
  <button id="home" type="button" onclick="location.href='index.html'">ANALYSIS</button>
  <button id="demo" type="button" onclick="location.href='demo.html'">DEMO</button>
</div>  
<div class="w3-container w3-text-theme">
  <br>
  <br>
    <h2 id="intro">Introduction</h2>
</div>
<p> Image captioning is an interesting area to investigate because it is a 
  combination of object detection and natural language processing, 
  which corresponds to the application of convolutional neural network (CNN) 
  and recurrent neural network (RNN). A good image captioning model mimics the 
  action of a human that it is able to understand and describe what an image includes. 
  In image captioning area, attention map is a widely used technique. With attention map, 
  the model knows which part in the image it needs to pay more attention to when generating 
  the next word in the caption. In our project, we also implemented 
  this attention map technique.  At the same time, it is important 
  that an image captioning model is robust, which means it could make reasonable 
  adjustments to the caption it generates when the input image is changed. 
  In our project, we also implemented an image perturbation model that alters 
  the input image to evaluate the robustness of our image captioning model.</p>

<br> <br> <br>
<div class="w3-container w3-text-theme">
  <h2 id="problem">The Problem</h2>
</div>


<p>Image captioning models are complex because they work on object detection as well as caption generation. When these models fail it is hard to understand where and why they fail. To explain how an image captioning model works, 
  we use attention maps to visualize the relationships between generated words and objects in an image.</p>
<br> <br> <br>


<div class="w3-container w3-text-theme">
  <h2 id="data">Data Description</h2>
</div>
<p>At the very beginning, we wanted to train the image captioning model with the combination of the COCO dataset and the Visual Genome dataset. After we realized it is hard to combine them, we decided to only use the COCO dataset to train this model. The reason why we use the COCO dataset in the end is that COCO is large and comprehensive enough to train our model. It contains 330k images and 5 captions per image. We use the CelebA (CelebFaces Attributes Dataset) dataset to train our image perturbation model. CelebA is a large-scale face attributes dataset with more than 200K celebrity images, 
  each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter.</p>
<br> <br>  <br>


<div class="w3-container w3-text-theme">
  <h2 id="data">Attention Maps</h2>
</div>
<p>With attention maps, we could visualize how our image captioning model generate that specific
  caption based on objects in an image. Let's take a look at an example of attention maps.</p>
<br><br>
<p>
  The generated caption is: <b>a small dog sitting on top of a toilet </b> 
</p>
<img id="demoImg" src="attentionmap_example.png" />
<br> <br><br>


<div class="w3-container w3-text-theme">
  <h2 id="data">Counterfactuals</h2>
</div>
<p>However, a generated caption can be significantly influenced by conterfactuals. Let's take a look at an example </p>
<br><br>
<img id="demoImg" src="counter.png" />
<br> <br><br>
<div class="w3-container w3-text-theme">
  <h2 id="data">Conclusion</h2>
</div>

<p>We trained our own image captioning model with the COCO dataset and evaluated the model with BLEU metric. 
  With the image captioning model, we generated attention maps to visualize and explain to the audience how a 
  caption is generated by our model step by step. We also implemented our image perturbation model and trained 
  it with the COCO dataset. It has decent performance on removing an object from an image and refilling it. 
  With the image perturbation model, we investigated how an caption can be changed if an object is removed 
  from the raw image. Furthermore, we investigated and visualized the object importance by assigning an 
  importance score to each object in an image. In the future, we want to make our model detect 
  adversarial and see how much the caption changes. We also plan to add a segmentation prediction 
  model to eliminate the need for pre-defined annotations. We hope our work will let more people know about Explainable AI.</p>
<br> <br><br>
</div>
</body>
<script src="main.js"></script> 
</html>